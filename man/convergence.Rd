\name{convergence}
\alias{convergence}
\title{Assessing Convergence for Fitted Models}

\description{
  The \pkg{lme4} package uses general-purpose nonlinear optimizers
  (e.g. Nelder-Mead or Powell's BOBYQA method) to estimate parameters.
  Assessing reliably whether such algorithms have converged is
  difficult.  For example, evaluating the
  \href{http://en.wikipedia.org/wiki/Karush\%E2\%80\%93Kuhn\%E2\%80\%93Tucker_conditions}{%
  Karush-Kuhn-Tucker conditions} (convergence criteria which reduce in the
  simplest case of non-constrained optimization to showing that
  the gradient is zero and the Hessian is positive definite) is
  challenging because of the difficulty of evaluating the gradient and
  Hessian.

  The current strategy for convergence testing is as follows:
  \itemize{
    \item report any convergence warnings generated by the optimizer
    itself
    \item compute the gradient by finite differences (FD), with
    a fixed step size of \eqn{10^{-4}}{1e-4} and, if the total
    number of observations < 8000, compute the Hessian by FD.
    (Our experiments have determined that above roughly this size, our
    naive FD calculations become unreliable.) Users can override
    this behaviour by setting the \code{calc.derivs} argument in
    \code{\link[=lmerControl]{(g)lmerControl}} explicitly, and may use the
    \code{deriv.method} argument to switch to slower but more
    accurate Richardson extrapolation using the \code{numDeriv} package.
    \item if the gradient and Hessian are available, compute the
    \emph{scaled} gradient, equivalent to scaling the gradient by
    the estimated Wald standard errors of the parameters. For each
    parameter, find the minimum of the absolute values of the
    absolute (unscaled) and scaled gradients; warn if the overall
    minumum value is >0.01.
    \item if the Hessian was not computed, or if the gradient scaling
    computation value failed, warn if the overall minimum value
    of the absolute value of the gradient is >0.05.
  }

  Warning tolerances were greatly increased in version 1.1-13, due to
  a large number of (apparently) false positive warnings.
  If you still see convergence warnings, and want to double-check
  the results, the following steps are recommended (examples are given below):
  \itemize{
    \item double-check the model specification and the data
    for mistakes
    \item center and scale continuous predictor variables (e.g. with
    \code{\link{scale}})
    \item check for singularity: if any of the diagonal elements of the
    Cholesky factor are zero or very small, the convergence testing methods may be
    inappropriate (see examples)
    \item double-check the Hessian calculation with the more expensive
    Richardson extrapolation method (see examples)
    \item restart the fit from the apparent optimum, or from a point
    perturbed slightly away from the optimum
    \item try all available optimizers (e.g. several different implementations
    of BOBYQA and Nelder-Mead, L-BFGS-B from \code{optim}, \code{nlminb},
    \dots)  While this will of course be slow for large fits, we consider
    it the gold standard; if all optimizers converge to values that
    are practically equivalent, then we would consider the convergence
    warnings to be false positives.
  }
  To quote Douglas Adams,
  \href{http://en.wikipedia.org/wiki/So_Long,_and_Thanks_for_All_the_Fish}{we
    apologize for the inconvenience}.
}% end{description}

\examples{
fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)

## 1. center and scale predictors:
ss.CS <- transform(sleepstudy, Days=scale(Days))
fm1.CS <- update(fm1, data=ss.CS)

## 2. check singularity
diag.vals <- getME(fm1,"theta")[getME(fm1,"lower") == 0]
any(diag.vals < 1e-6) # FALSE

## 3. recompute gradient and Hessian with Richardson extrapolation
fm2 <- update(fm1, control=lmerControl(deriv.method="Richardson"))
cat("hess:\n"); print(H <- fm2@optinfo$derivs$Hessian)
cat("grad:\n"); print(g <- fm2@optinfo$derivs$gradient)
cat("scaled gradient:\n")
print(scgrad <- c(solve(chol(H), g)))

## compare with naive FD results
fm1@optinfo$derivs

## 4. restart the fit from the original value (or
## a slightly perturbed value):
fm1.restart <- update(fm1, start=pars)

## 5. try all available optimizers

  source(system.file("utils", "allFit.R", package="lme4"))
  fm1.all <- allFit(fm1)
  ss <- summary(fm1.all)
  ss$ fixef               ## extract fixed effects
  ss$ llik                ## log-likelihoods
  ss$ sdcor               ## SDs and correlations
  ss$ theta               ## Cholesky factors
  ss$ which.OK            ## which fits worked

}

\seealso{\code{\link{lmerControl}}}
